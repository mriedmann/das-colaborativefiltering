{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitfc76f8d388764e2f87f6147686064c95",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colaborative Filtering\n",
    "\n",
    "\n",
    "https://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "MovieLens data sets were collected by the GroupLens Research Project\n",
    "at the University of Minnesota.\n",
    "\n",
    "This data set consists of:\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies.\n",
    "* Each user has rated at least 20 movies.\n",
    "* Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens web site\n",
    "(movielens.umn.edu) during the seven-month period from September 19th,\n",
    "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
    "who had less than 20 ratings or did not have complete demographic\n",
    "information were removed from this data set. Detailed descriptions of\n",
    "the data file can be found at the end of this file.\n",
    "\n",
    "source: http://files.grouplens.org/datasets/movielens/ml-100k-README.txt\n",
    "\n",
    "Memory-Based Collaborative Filtering approaches can be divided into two main sections: user-item filtering and item-item filtering. A user-item filtering takes a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. In contrast, item-item filtering will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations.\n",
    "Item-Item Collaborative Filtering: “Users who liked this item also liked …”\n",
    "User-Item Collaborative Filtering: “Users who are similar to you also liked …”\n",
    "\n",
    "source: https://blog.cambridgespark.com/nowadays-recommender-systems-are-used-to-personalize-your-experience-on-the-web-telling-you-what-120f39b89c3c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       user_id  item_id  rating  timestamp\n0          196      242       3  881250949\n1          186      302       3  891717742\n2           22      377       1  878887116\n3          244       51       2  880606923\n4          166      346       1  886397596\n...        ...      ...     ...        ...\n99995      880      476       3  880175444\n99996      716      204       5  879795543\n99997      276     1090       1  874795795\n99998       13      225       2  882399156\n99999       12      203       3  879959583\n\n[100000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n      <td>881250949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n      <td>891717742</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n      <td>878887116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n      <td>880606923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n      <td>886397596</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>880</td>\n      <td>476</td>\n      <td>3</td>\n      <td>880175444</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>716</td>\n      <td>204</td>\n      <td>5</td>\n      <td>879795543</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>276</td>\n      <td>1090</td>\n      <td>1</td>\n      <td>874795795</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>13</td>\n      <td>225</td>\n      <td>2</td>\n      <td>882399156</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>12</td>\n      <td>203</td>\n      <td>3</td>\n      <td>879959583</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_attributes=[\"user_id\",\"item_id\",\"rating\",\"timestamp\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\",\n",
    "    names=data_attributes,\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'n_users': 943, 'n_items': 1682}"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "info = { \n",
    "    \"n_users\": df.user_id.unique().shape[0],\n",
    "    \"n_items\": df.item_id.unique().shape[0]\n",
    "}\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       user_id  item_id  rating  timestamp\n32339      100      349       3  891375629\n11744      389     1530       2  880088753\n68093      896      880       4  887235664\n76468      765       50       2  880346255\n89963      689      358       4  876674762\n...        ...      ...     ...        ...\n64121       18       52       5  880130680\n42328      514       19       4  875463128\n82118      894     1404       3  882404536\n6691       189     1098       4  893265506\n88058      450      498       3  882396351\n\n[80000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>32339</th>\n      <td>100</td>\n      <td>349</td>\n      <td>3</td>\n      <td>891375629</td>\n    </tr>\n    <tr>\n      <th>11744</th>\n      <td>389</td>\n      <td>1530</td>\n      <td>2</td>\n      <td>880088753</td>\n    </tr>\n    <tr>\n      <th>68093</th>\n      <td>896</td>\n      <td>880</td>\n      <td>4</td>\n      <td>887235664</td>\n    </tr>\n    <tr>\n      <th>76468</th>\n      <td>765</td>\n      <td>50</td>\n      <td>2</td>\n      <td>880346255</td>\n    </tr>\n    <tr>\n      <th>89963</th>\n      <td>689</td>\n      <td>358</td>\n      <td>4</td>\n      <td>876674762</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>64121</th>\n      <td>18</td>\n      <td>52</td>\n      <td>5</td>\n      <td>880130680</td>\n    </tr>\n    <tr>\n      <th>42328</th>\n      <td>514</td>\n      <td>19</td>\n      <td>4</td>\n      <td>875463128</td>\n    </tr>\n    <tr>\n      <th>82118</th>\n      <td>894</td>\n      <td>1404</td>\n      <td>3</td>\n      <td>882404536</td>\n    </tr>\n    <tr>\n      <th>6691</th>\n      <td>189</td>\n      <td>1098</td>\n      <td>4</td>\n      <td>893265506</td>\n    </tr>\n    <tr>\n      <th>88058</th>\n      <td>450</td>\n      <td>498</td>\n      <td>3</td>\n      <td>882396351</td>\n    </tr>\n  </tbody>\n</table>\n<p>80000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(80000,)"
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "train_data_matrix = np.zeros((info[\"n_users\"], info[\"n_items\"]))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "\n",
    "tt = train_data_matrix[train_data_matrix != 0]\n",
    "#train_data_matrix[train_data_matrix.nonzero()].flatten().shape\n",
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1584444"
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "x = test_data_pivot.values.flatten()\n",
    "s = x != 0\n",
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((943, 1682), (943, 1682))"
     },
     "metadata": {},
     "execution_count": 215
    }
   ],
   "source": [
    "# create user-item matrix as pivot table\n",
    "train_data_pivot = train_data.pivot_table(index='user_id', columns='item_id', values='rating', fill_value=0)\\\n",
    "    .reindex(sorted(df.user_id.unique()), axis=0, fill_value=0)\\\n",
    "    .reindex(sorted(df.item_id.unique()), axis=1, fill_value=0)\n",
    "\n",
    "# create testset\n",
    "test_data_pivot = test_data.pivot_table(index='user_id', columns='item_id', values='rating', fill_value=0)\\\n",
    "    .reindex(sorted(df.user_id.unique()), axis=0, fill_value=0)\\\n",
    "    .reindex(sorted(df.item_id.unique()), axis=1, fill_value=0)\n",
    "\n",
    "(train_data_pivot.shape, test_data_pivot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((943, 943), (1682, 1682))"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "user_similarity = pairwise_distances(train_data_pivot, metric=\"cosine\")\n",
    "item_similarity = pairwise_distances(train_data_pivot.transpose(), metric=\"cosine\")\n",
    "\n",
    "(user_similarity.shape, item_similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             0         1         2         3         4         5         6     \\\nuser_id                                                                         \n1        1.640053  0.598829  0.493651  0.800931  0.493227  0.359747  1.485463   \n2        1.403151  0.330732  0.163136  0.573692  0.181128  0.012386  1.257005   \n3        1.407865  0.288126  0.137114  0.537502  0.142462 -0.015819  1.243605   \n4        1.361413  0.256854  0.109691  0.500590  0.116689 -0.038976  1.202659   \n5        1.448761  0.403349  0.298807  0.624226  0.299459  0.170374  1.312081   \n...           ...       ...       ...       ...       ...       ...       ...   \n939      1.334669  0.302718  0.151884  0.544064  0.162487  0.011750  1.211854   \n940      1.461572  0.375370  0.246547  0.590738  0.250927  0.101972  1.285553   \n941      1.258534  0.238670  0.081863  0.483019  0.097587 -0.059191  1.116564   \n942      1.441377  0.356062  0.230834  0.586993  0.232825  0.081489  1.301711   \n943      1.474138  0.401298  0.307156  0.625252  0.304786  0.185660  1.323953   \n\n             7         8         9     ...      1672      1673      1674  \\\nuser_id                                ...                                 \n1        0.903942  1.203040  0.540702  ...  0.284774  0.285341  0.284782   \n2        0.662912  0.908787  0.213074  ... -0.067401 -0.066187 -0.067415   \n3        0.631795  0.914709  0.189250  ... -0.102729 -0.101485 -0.103339   \n4        0.594614  0.878623  0.166354  ... -0.124717 -0.123602 -0.124756   \n5        0.719122  1.048874  0.355544  ...  0.086003  0.086609  0.086068   \n...           ...       ...       ...  ...       ...       ...       ...   \n939      0.634021  0.868963  0.208778  ... -0.071567 -0.070298 -0.071884   \n940      0.674008  0.973114  0.294906  ...  0.019349  0.020250  0.019501   \n941      0.574984  0.837876  0.141187  ... -0.143888 -0.142798 -0.144034   \n942      0.665339  0.973322  0.276177  ... -0.001875 -0.001145 -0.001621   \n943      0.736962  1.034549  0.375627  ...  0.100291  0.101099  0.100352   \n\n             1675      1676      1677      1678      1679      1680      1681  \nuser_id                                                                        \n1        0.283776  0.284285  0.283055  0.285640  0.284347  0.281762  0.284004  \n2       -0.068499 -0.067682 -0.069612 -0.067501 -0.068556 -0.070667 -0.067304  \n3       -0.104259 -0.102940 -0.105411 -0.104036 -0.104724 -0.106099 -0.102756  \n4       -0.125805 -0.124821 -0.127072 -0.125410 -0.126241 -0.127903 -0.124632  \n5        0.084993  0.085906  0.084082  0.086562  0.085322  0.082843  0.085485  \n...           ...       ...       ...       ...       ...       ...       ...  \n939     -0.072843 -0.071749 -0.073618 -0.071335 -0.072477 -0.074760 -0.071551  \n940      0.018441  0.019248  0.017371  0.019474  0.018422  0.016319  0.019231  \n941     -0.145142 -0.144468 -0.146269 -0.144092 -0.145181 -0.147358 -0.144027  \n942     -0.002659 -0.001763 -0.003578 -0.001266 -0.002422 -0.004734 -0.001588  \n943      0.099318  0.100135  0.098503  0.101013  0.099758  0.097249  0.099563  \n\n[943 rows x 1682 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1672</th>\n      <th>1673</th>\n      <th>1674</th>\n      <th>1675</th>\n      <th>1676</th>\n      <th>1677</th>\n      <th>1678</th>\n      <th>1679</th>\n      <th>1680</th>\n      <th>1681</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1.640053</td>\n      <td>0.598829</td>\n      <td>0.493651</td>\n      <td>0.800931</td>\n      <td>0.493227</td>\n      <td>0.359747</td>\n      <td>1.485463</td>\n      <td>0.903942</td>\n      <td>1.203040</td>\n      <td>0.540702</td>\n      <td>...</td>\n      <td>0.284774</td>\n      <td>0.285341</td>\n      <td>0.284782</td>\n      <td>0.283776</td>\n      <td>0.284285</td>\n      <td>0.283055</td>\n      <td>0.285640</td>\n      <td>0.284347</td>\n      <td>0.281762</td>\n      <td>0.284004</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.403151</td>\n      <td>0.330732</td>\n      <td>0.163136</td>\n      <td>0.573692</td>\n      <td>0.181128</td>\n      <td>0.012386</td>\n      <td>1.257005</td>\n      <td>0.662912</td>\n      <td>0.908787</td>\n      <td>0.213074</td>\n      <td>...</td>\n      <td>-0.067401</td>\n      <td>-0.066187</td>\n      <td>-0.067415</td>\n      <td>-0.068499</td>\n      <td>-0.067682</td>\n      <td>-0.069612</td>\n      <td>-0.067501</td>\n      <td>-0.068556</td>\n      <td>-0.070667</td>\n      <td>-0.067304</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.407865</td>\n      <td>0.288126</td>\n      <td>0.137114</td>\n      <td>0.537502</td>\n      <td>0.142462</td>\n      <td>-0.015819</td>\n      <td>1.243605</td>\n      <td>0.631795</td>\n      <td>0.914709</td>\n      <td>0.189250</td>\n      <td>...</td>\n      <td>-0.102729</td>\n      <td>-0.101485</td>\n      <td>-0.103339</td>\n      <td>-0.104259</td>\n      <td>-0.102940</td>\n      <td>-0.105411</td>\n      <td>-0.104036</td>\n      <td>-0.104724</td>\n      <td>-0.106099</td>\n      <td>-0.102756</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.361413</td>\n      <td>0.256854</td>\n      <td>0.109691</td>\n      <td>0.500590</td>\n      <td>0.116689</td>\n      <td>-0.038976</td>\n      <td>1.202659</td>\n      <td>0.594614</td>\n      <td>0.878623</td>\n      <td>0.166354</td>\n      <td>...</td>\n      <td>-0.124717</td>\n      <td>-0.123602</td>\n      <td>-0.124756</td>\n      <td>-0.125805</td>\n      <td>-0.124821</td>\n      <td>-0.127072</td>\n      <td>-0.125410</td>\n      <td>-0.126241</td>\n      <td>-0.127903</td>\n      <td>-0.124632</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.448761</td>\n      <td>0.403349</td>\n      <td>0.298807</td>\n      <td>0.624226</td>\n      <td>0.299459</td>\n      <td>0.170374</td>\n      <td>1.312081</td>\n      <td>0.719122</td>\n      <td>1.048874</td>\n      <td>0.355544</td>\n      <td>...</td>\n      <td>0.086003</td>\n      <td>0.086609</td>\n      <td>0.086068</td>\n      <td>0.084993</td>\n      <td>0.085906</td>\n      <td>0.084082</td>\n      <td>0.086562</td>\n      <td>0.085322</td>\n      <td>0.082843</td>\n      <td>0.085485</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>939</th>\n      <td>1.334669</td>\n      <td>0.302718</td>\n      <td>0.151884</td>\n      <td>0.544064</td>\n      <td>0.162487</td>\n      <td>0.011750</td>\n      <td>1.211854</td>\n      <td>0.634021</td>\n      <td>0.868963</td>\n      <td>0.208778</td>\n      <td>...</td>\n      <td>-0.071567</td>\n      <td>-0.070298</td>\n      <td>-0.071884</td>\n      <td>-0.072843</td>\n      <td>-0.071749</td>\n      <td>-0.073618</td>\n      <td>-0.071335</td>\n      <td>-0.072477</td>\n      <td>-0.074760</td>\n      <td>-0.071551</td>\n    </tr>\n    <tr>\n      <th>940</th>\n      <td>1.461572</td>\n      <td>0.375370</td>\n      <td>0.246547</td>\n      <td>0.590738</td>\n      <td>0.250927</td>\n      <td>0.101972</td>\n      <td>1.285553</td>\n      <td>0.674008</td>\n      <td>0.973114</td>\n      <td>0.294906</td>\n      <td>...</td>\n      <td>0.019349</td>\n      <td>0.020250</td>\n      <td>0.019501</td>\n      <td>0.018441</td>\n      <td>0.019248</td>\n      <td>0.017371</td>\n      <td>0.019474</td>\n      <td>0.018422</td>\n      <td>0.016319</td>\n      <td>0.019231</td>\n    </tr>\n    <tr>\n      <th>941</th>\n      <td>1.258534</td>\n      <td>0.238670</td>\n      <td>0.081863</td>\n      <td>0.483019</td>\n      <td>0.097587</td>\n      <td>-0.059191</td>\n      <td>1.116564</td>\n      <td>0.574984</td>\n      <td>0.837876</td>\n      <td>0.141187</td>\n      <td>...</td>\n      <td>-0.143888</td>\n      <td>-0.142798</td>\n      <td>-0.144034</td>\n      <td>-0.145142</td>\n      <td>-0.144468</td>\n      <td>-0.146269</td>\n      <td>-0.144092</td>\n      <td>-0.145181</td>\n      <td>-0.147358</td>\n      <td>-0.144027</td>\n    </tr>\n    <tr>\n      <th>942</th>\n      <td>1.441377</td>\n      <td>0.356062</td>\n      <td>0.230834</td>\n      <td>0.586993</td>\n      <td>0.232825</td>\n      <td>0.081489</td>\n      <td>1.301711</td>\n      <td>0.665339</td>\n      <td>0.973322</td>\n      <td>0.276177</td>\n      <td>...</td>\n      <td>-0.001875</td>\n      <td>-0.001145</td>\n      <td>-0.001621</td>\n      <td>-0.002659</td>\n      <td>-0.001763</td>\n      <td>-0.003578</td>\n      <td>-0.001266</td>\n      <td>-0.002422</td>\n      <td>-0.004734</td>\n      <td>-0.001588</td>\n    </tr>\n    <tr>\n      <th>943</th>\n      <td>1.474138</td>\n      <td>0.401298</td>\n      <td>0.307156</td>\n      <td>0.625252</td>\n      <td>0.304786</td>\n      <td>0.185660</td>\n      <td>1.323953</td>\n      <td>0.736962</td>\n      <td>1.034549</td>\n      <td>0.375627</td>\n      <td>...</td>\n      <td>0.100291</td>\n      <td>0.101099</td>\n      <td>0.100352</td>\n      <td>0.099318</td>\n      <td>0.100135</td>\n      <td>0.098503</td>\n      <td>0.101013</td>\n      <td>0.099758</td>\n      <td>0.097249</td>\n      <td>0.099563</td>\n    </tr>\n  </tbody>\n</table>\n<p>943 rows × 1682 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "source": [
    "def predict_user(ratings, similarity):\n",
    "    mean_user_rating = ratings.mean(axis=1)\n",
    "    rating_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "    df = pd.DataFrame(mean_user_rating[:, np.newaxis] + similarity.dot(rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.index.name=\"user_id\"\n",
    "    return df\n",
    "\n",
    "user_prediction = predict_user(train_data_pivot, user_similarity)\n",
    "user_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             0         1         2         3         4         5         6     \\\nuser_id                                                                         \n1        0.358834  0.377382  0.401504  0.357170  0.395166  0.416901  0.357030   \n2        0.092469  0.107400  0.102130  0.101950  0.104833  0.104374  0.092949   \n3        0.072092  0.075458  0.073512  0.074870  0.073448  0.075228  0.070134   \n4        0.047534  0.050011  0.049081  0.049538  0.049452  0.051132  0.046828   \n5        0.196575  0.200997  0.222632  0.197230  0.218662  0.245415  0.199102   \n...           ...       ...       ...       ...       ...       ...       ...   \n939      0.080410  0.093542  0.092610  0.092566  0.091428  0.099818  0.084185   \n940      0.150460  0.163437  0.171301  0.151123  0.169882  0.180040  0.146964   \n941      0.021619  0.028458  0.027221  0.027366  0.028546  0.030756  0.021521   \n942      0.139024  0.148967  0.158869  0.143440  0.155918  0.161287  0.142535   \n943      0.205866  0.196817  0.226557  0.200054  0.219257  0.258637  0.205444   \n\n             7         8         9     ...      1672      1673      1674  \\\nuser_id                                ...                                 \n1        0.372450  0.374311  0.400413  ...  0.438733  0.435869  0.440376   \n2        0.099913  0.093648  0.101149  ...  0.109001  0.109569  0.109131   \n3        0.074370  0.072218  0.073801  ...  0.074994  0.075540  0.072180   \n4        0.049522  0.048530  0.050698  ...  0.050912  0.050888  0.050780   \n5        0.206164  0.217225  0.229643  ...  0.245123  0.243313  0.246491   \n...           ...       ...       ...  ...       ...       ...       ...   \n939      0.092976  0.083682  0.097405  ...  0.100565  0.101404  0.098489   \n940      0.150933  0.153719  0.169427  ...  0.183904  0.183193  0.185488   \n941      0.027421  0.025811  0.029084  ...  0.031583  0.031633  0.031395   \n942      0.139401  0.144274  0.152912  ...  0.163488  0.161552  0.165183   \n943      0.216680  0.218420  0.245688  ...  0.257524  0.257024  0.258357   \n\n             1675      1676      1677      1678      1679      1680      1681  \nuser_id                                                                        \n1        0.440376  0.435737  0.445237  0.445237  0.445237  0.441736  0.430870  \n2        0.109131  0.108177  0.109292  0.109292  0.109292  0.109988  0.109876  \n3        0.072180  0.074357  0.070666  0.070666  0.070666  0.074911  0.075449  \n4        0.050780  0.050310  0.047402  0.047402  0.047402  0.051130  0.051348  \n5        0.246491  0.245431  0.248351  0.248351  0.248351  0.246136  0.242437  \n...           ...       ...       ...       ...       ...       ...       ...  \n939      0.098489  0.099818  0.101623  0.101623  0.101623  0.101070  0.101060  \n940      0.185488  0.183795  0.185126  0.185126  0.185126  0.186088  0.183367  \n941      0.031395  0.030543  0.031480  0.031480  0.031480  0.031510  0.031498  \n942      0.165183  0.163986  0.166601  0.166601  0.166601  0.165874  0.165615  \n943      0.258357  0.257245  0.261567  0.261567  0.261567  0.259215  0.251633  \n\n[943 rows x 1682 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1672</th>\n      <th>1673</th>\n      <th>1674</th>\n      <th>1675</th>\n      <th>1676</th>\n      <th>1677</th>\n      <th>1678</th>\n      <th>1679</th>\n      <th>1680</th>\n      <th>1681</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.358834</td>\n      <td>0.377382</td>\n      <td>0.401504</td>\n      <td>0.357170</td>\n      <td>0.395166</td>\n      <td>0.416901</td>\n      <td>0.357030</td>\n      <td>0.372450</td>\n      <td>0.374311</td>\n      <td>0.400413</td>\n      <td>...</td>\n      <td>0.438733</td>\n      <td>0.435869</td>\n      <td>0.440376</td>\n      <td>0.440376</td>\n      <td>0.435737</td>\n      <td>0.445237</td>\n      <td>0.445237</td>\n      <td>0.445237</td>\n      <td>0.441736</td>\n      <td>0.430870</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.092469</td>\n      <td>0.107400</td>\n      <td>0.102130</td>\n      <td>0.101950</td>\n      <td>0.104833</td>\n      <td>0.104374</td>\n      <td>0.092949</td>\n      <td>0.099913</td>\n      <td>0.093648</td>\n      <td>0.101149</td>\n      <td>...</td>\n      <td>0.109001</td>\n      <td>0.109569</td>\n      <td>0.109131</td>\n      <td>0.109131</td>\n      <td>0.108177</td>\n      <td>0.109292</td>\n      <td>0.109292</td>\n      <td>0.109292</td>\n      <td>0.109988</td>\n      <td>0.109876</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.072092</td>\n      <td>0.075458</td>\n      <td>0.073512</td>\n      <td>0.074870</td>\n      <td>0.073448</td>\n      <td>0.075228</td>\n      <td>0.070134</td>\n      <td>0.074370</td>\n      <td>0.072218</td>\n      <td>0.073801</td>\n      <td>...</td>\n      <td>0.074994</td>\n      <td>0.075540</td>\n      <td>0.072180</td>\n      <td>0.072180</td>\n      <td>0.074357</td>\n      <td>0.070666</td>\n      <td>0.070666</td>\n      <td>0.070666</td>\n      <td>0.074911</td>\n      <td>0.075449</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.047534</td>\n      <td>0.050011</td>\n      <td>0.049081</td>\n      <td>0.049538</td>\n      <td>0.049452</td>\n      <td>0.051132</td>\n      <td>0.046828</td>\n      <td>0.049522</td>\n      <td>0.048530</td>\n      <td>0.050698</td>\n      <td>...</td>\n      <td>0.050912</td>\n      <td>0.050888</td>\n      <td>0.050780</td>\n      <td>0.050780</td>\n      <td>0.050310</td>\n      <td>0.047402</td>\n      <td>0.047402</td>\n      <td>0.047402</td>\n      <td>0.051130</td>\n      <td>0.051348</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.196575</td>\n      <td>0.200997</td>\n      <td>0.222632</td>\n      <td>0.197230</td>\n      <td>0.218662</td>\n      <td>0.245415</td>\n      <td>0.199102</td>\n      <td>0.206164</td>\n      <td>0.217225</td>\n      <td>0.229643</td>\n      <td>...</td>\n      <td>0.245123</td>\n      <td>0.243313</td>\n      <td>0.246491</td>\n      <td>0.246491</td>\n      <td>0.245431</td>\n      <td>0.248351</td>\n      <td>0.248351</td>\n      <td>0.248351</td>\n      <td>0.246136</td>\n      <td>0.242437</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>939</th>\n      <td>0.080410</td>\n      <td>0.093542</td>\n      <td>0.092610</td>\n      <td>0.092566</td>\n      <td>0.091428</td>\n      <td>0.099818</td>\n      <td>0.084185</td>\n      <td>0.092976</td>\n      <td>0.083682</td>\n      <td>0.097405</td>\n      <td>...</td>\n      <td>0.100565</td>\n      <td>0.101404</td>\n      <td>0.098489</td>\n      <td>0.098489</td>\n      <td>0.099818</td>\n      <td>0.101623</td>\n      <td>0.101623</td>\n      <td>0.101623</td>\n      <td>0.101070</td>\n      <td>0.101060</td>\n    </tr>\n    <tr>\n      <th>940</th>\n      <td>0.150460</td>\n      <td>0.163437</td>\n      <td>0.171301</td>\n      <td>0.151123</td>\n      <td>0.169882</td>\n      <td>0.180040</td>\n      <td>0.146964</td>\n      <td>0.150933</td>\n      <td>0.153719</td>\n      <td>0.169427</td>\n      <td>...</td>\n      <td>0.183904</td>\n      <td>0.183193</td>\n      <td>0.185488</td>\n      <td>0.185488</td>\n      <td>0.183795</td>\n      <td>0.185126</td>\n      <td>0.185126</td>\n      <td>0.185126</td>\n      <td>0.186088</td>\n      <td>0.183367</td>\n    </tr>\n    <tr>\n      <th>941</th>\n      <td>0.021619</td>\n      <td>0.028458</td>\n      <td>0.027221</td>\n      <td>0.027366</td>\n      <td>0.028546</td>\n      <td>0.030756</td>\n      <td>0.021521</td>\n      <td>0.027421</td>\n      <td>0.025811</td>\n      <td>0.029084</td>\n      <td>...</td>\n      <td>0.031583</td>\n      <td>0.031633</td>\n      <td>0.031395</td>\n      <td>0.031395</td>\n      <td>0.030543</td>\n      <td>0.031480</td>\n      <td>0.031480</td>\n      <td>0.031480</td>\n      <td>0.031510</td>\n      <td>0.031498</td>\n    </tr>\n    <tr>\n      <th>942</th>\n      <td>0.139024</td>\n      <td>0.148967</td>\n      <td>0.158869</td>\n      <td>0.143440</td>\n      <td>0.155918</td>\n      <td>0.161287</td>\n      <td>0.142535</td>\n      <td>0.139401</td>\n      <td>0.144274</td>\n      <td>0.152912</td>\n      <td>...</td>\n      <td>0.163488</td>\n      <td>0.161552</td>\n      <td>0.165183</td>\n      <td>0.165183</td>\n      <td>0.163986</td>\n      <td>0.166601</td>\n      <td>0.166601</td>\n      <td>0.166601</td>\n      <td>0.165874</td>\n      <td>0.165615</td>\n    </tr>\n    <tr>\n      <th>943</th>\n      <td>0.205866</td>\n      <td>0.196817</td>\n      <td>0.226557</td>\n      <td>0.200054</td>\n      <td>0.219257</td>\n      <td>0.258637</td>\n      <td>0.205444</td>\n      <td>0.216680</td>\n      <td>0.218420</td>\n      <td>0.245688</td>\n      <td>...</td>\n      <td>0.257524</td>\n      <td>0.257024</td>\n      <td>0.258357</td>\n      <td>0.258357</td>\n      <td>0.257245</td>\n      <td>0.261567</td>\n      <td>0.261567</td>\n      <td>0.261567</td>\n      <td>0.259215</td>\n      <td>0.251633</td>\n    </tr>\n  </tbody>\n</table>\n<p>943 rows × 1682 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 219
    }
   ],
   "source": [
    "def predict_item(ratings, similarity):\n",
    "    return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "\n",
    "item_prediction = predict_item(train_data_pivot, item_similarity)\n",
    "item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'user_prediction_rmse': 3.1043685831632986,\n 'item_prediction_rmse': 3.451138462140362}"
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(predicion, groud_truth):\n",
    "    groud_truth = groud_truth.values.flatten()\n",
    "    s = groud_truth != 0\n",
    "    groud_truth = groud_truth[s]\n",
    "    predicion = predicion.values.flatten()\n",
    "    predicion = predicion[s]\n",
    "    \n",
    "    return sqrt(mean_squared_error(predicion, groud_truth))\n",
    "\n",
    "{'user_prediction_rmse': rmse(user_prediction, test_data_pivot), 'item_prediction_rmse':  rmse(item_prediction, test_data_pivot)}"
   ]
  }
 ]
}