{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colaborative Filtering\n",
    "\n",
    "Collaborative filtering is a way recommendation systems filter information by using the preferences of other people. It uses the assumption that if person A has similar preferences to person B on items they have both reviewed, then person A is likely to have a similar preference to person B on an item only person B has reviewed.\n",
    "\n",
    "Collaborative filtering is used by many recommendation systems in various fields, including music, shopping, financial data, and social networks and by various services (YouTube, Reddit, Last.fm). Any service that uses a recommendation system most likely employs collaborative filtering.\n",
    "\n",
    "source: https://brilliant.org/wiki/collaborative-filtering/\n",
    "\n",
    "## Memory-based CF\n",
    "\n",
    "Memory-Based Collaborative Filtering approaches can be divided into two main sections: user-item (user-based) filtering and item-item (item-based) filtering. A user-item filtering takes a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. In contrast, item-item filtering will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations.\n",
    "\n",
    "Item-Item Collaborative Filtering: “Users who liked this item also liked …”\n",
    "\n",
    "User-Item Collaborative Filtering: “Users who are similar to you also liked …”\n",
    "\n",
    "source: https://blog.cambridgespark.com/nowadays-recommender-systems-are-used-to-personalize-your-experience-on-the-web-telling-you-what-120f39b89c3c\n",
    "\n",
    "## Model-baesd CF\n",
    "\n",
    "Model-based Collaborative Filtering is based on matrix factorization (MF) which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF. The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization you can restructure the user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector. You fit this matrix to approximate your original matrix, as closely as possible, by multiplying the low-rank matrices together, which fills in the entries missing in the original matrix.\n",
    "\n",
    "source: https://blog.cambridgespark.com/nowadays-recommender-systems-are-used-to-personalize-your-experience-on-the-web-telling-you-what-120f39b89c3c\n",
    "\n",
    "## Task\n",
    "\n",
    "In this exercise, you shall work with various collaborative filtering approaches. Specifically, you shall compare\n",
    "\n",
    "* User based filtering\n",
    "* Item based filtering\n",
    "* One other model based filtering approach\n",
    "\n",
    "You can reuse existing libraries and code examples (if you do so, please properly quote the origin, otherwise it has to be considered plagiarism), and you shall compare their performance in two ways\n",
    "\n",
    "* Effectiveness of the recommendation on a supplied training set.\n",
    "* Efficiency of the recommendation (i.e. runtime).\n",
    "\n",
    "The dataset to be used is the MovieLens dataset. You shall first work with the smallest version available, with 100k ratings, at https://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "To ensure that we can compare the results across all your peers in the course, you shall proceed as follows\n",
    "\n",
    "* Split the dataset into 80:20 training:test set, after shuffling the data. \n",
    "* For evaluation of effectiveness, we will utilise MSE.\n",
    "\n",
    "Your solution shall include the code, and a report on your findings - which methods worked well in regards to effectiveness and efficiency? Are the result in general usable?\n",
    "\n",
    "After the first step on the 100k database, obtain the next bigger version (1M), and just test your algorithms for effectiveness - do the methods scale to the increased size?\n",
    "\n",
    "## Dataset description\n",
    "\n",
    "For this exercise 2 movielense datasets were used: 100k (https://grouplens.org/datasets/movielens/100k/) and 1M (https://grouplens.org/datasets/movielens/1M/)\n",
    "\n",
    "MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.\n",
    "\n",
    "The 100k data set consists of:\n",
    "* 100,000 ratings (1-5) \n",
    "* from 943 users \n",
    "* on 1682 movies.\n",
    "\n",
    "The 1m data set consists of:\n",
    "* 1,000,209 ratings (1-5) \n",
    "* from 6,040 users \n",
    "* on 3,952 movies.  \n",
    "\n",
    "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set.\n",
    "\n",
    "sources: http://files.grouplens.org/datasets/movielens/ml-100k-README.txt http://files.grouplens.org/datasets/movielens/ml-1m-README.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hashlib import md5\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "data_attributes=[\"user_id\",\"item_id\",\"rating\",\"timestamp\"]\n",
    "\n",
    "def get_content_file(url):\n",
    "    h = md5(url.encode()).hexdigest()\n",
    "    path = \".tmp/\" + h\n",
    "    file_path = path + \"/data.zip\"\n",
    "    if not os.path.exists(file_path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    return file_path\n",
    "\n",
    "file_path = get_content_file(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\")\n",
    "with ZipFile(file_path).open(\"ml-1m/ratings.dat\") as decompressed_file:\n",
    "    df_1m = pd.read_csv(\n",
    "        decompressed_file,\n",
    "        names=data_attributes,\n",
    "        sep=\"::\"\n",
    "    )\n",
    "\n",
    "file_path = get_content_file(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\")\n",
    "with ZipFile(file_path).open(\"ml-100k/u.data\") as decompressed_file:\n",
    "    df_100k = pd.read_csv(\n",
    "        decompressed_file,\n",
    "        names=data_attributes,\n",
    "        sep=\"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((6040, 3706), (6040, 3706))"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_pivot_dataset(df, test_size):\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size)\n",
    "\n",
    "    # create user-item matrix as pivot table\n",
    "    train_data_pivot = train_data.pivot_table(index='user_id', columns='item_id', values='rating', fill_value=0)\\\n",
    "        .reindex(sorted(df.user_id.unique()), axis=0, fill_value=0)\\\n",
    "        .reindex(sorted(df.item_id.unique()), axis=1, fill_value=0)\n",
    "\n",
    "    # create testset\n",
    "    test_data_pivot = test_data.pivot_table(index='user_id', columns='item_id', values='rating', fill_value=0)\\\n",
    "        .reindex(sorted(df.user_id.unique()), axis=0, fill_value=0)\\\n",
    "        .reindex(sorted(df.item_id.unique()), axis=1, fill_value=0)\n",
    "    \n",
    "    return (train_data_pivot, test_data_pivot)\n",
    "\n",
    "# Split into train and test-set (20% = 80:20) and create \"user x item = rating\" pivot table\n",
    "train_data_pivot, test_data_pivot = split_and_pivot_dataset(df_1m, 0.2)\n",
    "(train_data_pivot.shape, test_data_pivot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.0180236 ,  0.20521667,  0.11805004, ..., -0.04492769,\n        -0.05070511,  0.12639953],\n       [ 1.09904824,  0.25906479,  0.17028461, ...,  0.00984377,\n         0.00480964,  0.17962567],\n       [ 1.03029924,  0.19528433,  0.10735116, ..., -0.05453308,\n        -0.05981754,  0.11935182],\n       ...,\n       [ 1.0227343 ,  0.17795679,  0.08507883, ..., -0.07942739,\n        -0.08546295,  0.09316322],\n       [ 1.09277612,  0.2570656 ,  0.16787987, ...,  0.0034236 ,\n        -0.00187697,  0.17576156],\n       [ 1.2404037 ,  0.41610148,  0.32872287, ...,  0.16246638,\n         0.15756822,  0.32905561]])"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def predict_user(ratings):\n",
    "    # calculate pairwise distances for items to users with cosine metric\n",
    "    similarity = pairwise_distances(train_data_pivot, metric=\"cosine\")\n",
    "    # calculate mean values \n",
    "    mean_user_rating = ratings.mean(axis=1)\n",
    "    # normalize ratings\n",
    "    rating_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "    # calculate user-item correlations\n",
    "    return mean_user_rating[:, np.newaxis] + similarity.dot(rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "\n",
    "user_prediction = predict_user(train_data_pivot)\n",
    "user_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.78 s ± 165 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "predict_user(train_data_pivot, user_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.03994096, 0.04597291, 0.04877483, ..., 0.0516772 , 0.0522447 ,\n        0.04912193],\n       [0.0881389 , 0.09520341, 0.09850234, ..., 0.10348557, 0.10546052,\n        0.09878872],\n       [0.03238386, 0.03634189, 0.03841949, ..., 0.04175259, 0.0428211 ,\n        0.03973853],\n       ...,\n       [0.01628693, 0.01805726, 0.01847296, ..., 0.01890494, 0.01916679,\n        0.01836872],\n       [0.08680269, 0.09385945, 0.09675042, ..., 0.09851305, 0.10017828,\n        0.09661184],\n       [0.22606293, 0.24504202, 0.25076236, ..., 0.25092862, 0.25267385,\n        0.24295449]])"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def predict_item(ratings):\n",
    "    # calculate pairwise distances for users to items with cosine metric\n",
    "    similarity = pairwise_distances(train_data_pivot.transpose(), metric=\"cosine\")\n",
    "    # calculate item-item correlations\n",
    "    return ratings.values.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "\n",
    "item_prediction = predict_item(train_data_pivot)\n",
    "item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3.49 s ± 126 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "predict_item(train_data_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 3.03404884e+00,  6.34649733e-01, -9.40281604e-03, ...,\n        -3.77338131e-03, -1.62436537e-02,  1.02307408e-01],\n       [ 8.18947825e-01,  3.54068048e-01,  2.12335766e-01, ...,\n         1.76069786e-02,  2.73976838e-02,  5.56844141e-02],\n       [ 7.17533577e-01,  1.12967827e-01,  1.03349885e-01, ...,\n        -2.44407009e-02, -2.67896375e-03, -4.12819454e-02],\n       ...,\n       [ 5.71280996e-01,  1.67304835e-02, -3.49634201e-02, ...,\n        -7.58731702e-03, -3.43104651e-04, -3.50520619e-02],\n       [ 8.27784594e-01,  2.23862533e-01,  2.08391561e-01, ...,\n         3.52896740e-02,  1.35098252e-02,  9.73627717e-03],\n       [ 1.41986585e+00,  2.27577113e-01, -3.36611543e-01, ...,\n         5.91306194e-02,  7.45368305e-02,  3.36650894e-01]])"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def predict_svd(ratings):\n",
    "    U, S, Vt = svds(ratings, k=20)\n",
    "    Sd = np.diag(S)\n",
    "    return np.dot(np.dot(U, Sd), Vt)\n",
    "\n",
    "model_prediction = predict_svd(train_data_pivot.values.astype(float))\n",
    "model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.66 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "predict_svd(train_data_pivot.values.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'user_prediction_mse': 10.356130342203791,\n 'item_prediction_mse': 12.321712953156142,\n 'model_prediction_mse': 7.324933251922383}"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def mse(predicion, groud_truth):\n",
    "    predicion = predicion[groud_truth != 0].flatten()\n",
    "    groud_truth = groud_truth[groud_truth != 0].flatten()\n",
    "    \n",
    "    return mean_squared_error(predicion, groud_truth)\n",
    "\n",
    "{\n",
    "    'user_prediction_mse': mse(user_prediction, test_data_pivot.values), \n",
    "    'item_prediction_mse':  mse(item_prediction, test_data_pivot.values),\n",
    "    'model_prediction_mse': mse(model_prediction, test_data_pivot.values)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitfc76f8d388764e2f87f6147686064c95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}