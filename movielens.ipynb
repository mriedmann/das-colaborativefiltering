{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colaborative Filtering\n",
    "\n",
    "Collaborative filtering is a way recommendation systems filter information by using the preferences of other people. It uses the assumption that if person A has similar preferences to person B on items they have both reviewed, then person A is likely to have a similar preference to person B on an item only person B has reviewed.\n",
    "\n",
    "Collaborative filtering is used by many recommendation systems in various fields, including music, shopping, financial data, and social networks and by various services (YouTube, Reddit, Last.fm). Any service that uses a recommendation system most likely employs collaborative filtering.\n",
    "\n",
    "source: https://brilliant.org/wiki/collaborative-filtering/\n",
    "\n",
    "## Memory-based CF\n",
    "\n",
    "Memory-Based Collaborative Filtering approaches can be divided into two main sections: user-item (user-based) filtering and item-item (item-based) filtering. A user-item filtering takes a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. In contrast, item-item filtering will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations.\n",
    "\n",
    "Item-Item Collaborative Filtering: “Users who liked this item also liked …”\n",
    "\n",
    "User-Item Collaborative Filtering: “Users who are similar to you also liked …”\n",
    "\n",
    "source: https://blog.cambridgespark.com/nowadays-recommender-systems-are-used-to-personalize-your-experience-on-the-web-telling-you-what-120f39b89c3c\n",
    "\n",
    "## Model-baesd CF\n",
    "\n",
    "Model-based Collaborative Filtering is based on matrix factorization (MF) which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF. The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization you can restructure the user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector. You fit this matrix to approximate your original matrix, as closely as possible, by multiplying the low-rank matrices together, which fills in the entries missing in the original matrix.\n",
    "\n",
    "source: https://blog.cambridgespark.com/nowadays-recommender-systems-are-used-to-personalize-your-experience-on-the-web-telling-you-what-120f39b89c3c\n",
    "\n",
    "## Task\n",
    "\n",
    "In this exercise, you shall work with various collaborative filtering approaches. Specifically, you shall compare\n",
    "\n",
    "* User based filtering\n",
    "* Item based filtering\n",
    "* One other model based filtering approach\n",
    "\n",
    "You can reuse existing libraries and code examples (if you do so, please properly quote the origin, otherwise it has to be considered plagiarism), and you shall compare their performance in two ways\n",
    "\n",
    "* Effectiveness of the recommendation on a supplied training set.\n",
    "* Efficiency of the recommendation (i.e. runtime).\n",
    "\n",
    "The dataset to be used is the MovieLens dataset. You shall first work with the smallest version available, with 100k ratings, at https://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "To ensure that we can compare the results across all your peers in the course, you shall proceed as follows\n",
    "\n",
    "* Split the dataset into 80:20 training:test set, after shuffling the data. \n",
    "* For evaluation of effectiveness, we will utilise MSE.\n",
    "\n",
    "Your solution shall include the code, and a report on your findings - which methods worked well in regards to effectiveness and efficiency? Are the result in general usable?\n",
    "\n",
    "After the first step on the 100k database, obtain the next bigger version (1M), and just test your algorithms for effectiveness - do the methods scale to the increased size?\n",
    "\n",
    "## Dataset description\n",
    "\n",
    "For this exercise 2 movielense datasets were used: 100k (https://grouplens.org/datasets/movielens/100k/) and 1M (https://grouplens.org/datasets/movielens/1M/)\n",
    "\n",
    "MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.\n",
    "\n",
    "The 100k data set consists of:\n",
    "* 100,000 ratings (1-5) \n",
    "* from 943 users \n",
    "* on 1682 movies.\n",
    "\n",
    "The 1m data set consists of:\n",
    "* 1,000,209 ratings (1-5) \n",
    "* from 6,040 users \n",
    "* on 3,952 movies.  \n",
    "\n",
    "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set.\n",
    "\n",
    "sources: http://files.grouplens.org/datasets/movielens/ml-100k-README.txt http://files.grouplens.org/datasets/movielens/ml-1m-README.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hashlib import md5\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "data_attributes=[\"user_id\",\"item_id\",\"rating\",\"timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aquicition and preperation\n",
    "\n",
    "First the dataset is downloaded as zip-archive. To save bandwidth ,time and space it is cached but not decompressed. Only the relevant data file inside the archive is read and stored in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_file(url):\n",
    "    h = md5(url.encode()).hexdigest()\n",
    "    path = \".tmp/\" + h\n",
    "    file_path = path + \"/data.zip\"\n",
    "    if not os.path.exists(file_path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "file_path = get_content_file(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\")\n",
    "with ZipFile(file_path).open(\"ml-100k/u.data\") as decompressed_file:\n",
    "    df_100k = pd.read_csv(\n",
    "        decompressed_file,\n",
    "        names=data_attributes,\n",
    "        sep=\"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make evaluating the predictions possible, the dataset is split in a 80:20 ratio and converted to a \"user x item = rating\" pivot table. As shown below, the distribution of the test and train set is almost identical. It can be assumed that both sets are representitve for the whole set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pivot_dataset(df, test_size):\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size)\n",
    "\n",
    "    # create user-item matrix as pivot table\n",
    "    train_data_pivot = train_data.pivot_table(index='user_id', columns='item_id', values='rating')\\\n",
    "        .reindex(sorted(df.user_id.unique()), axis=0)\\\n",
    "        .reindex(sorted(df.item_id.unique()), axis=1)\n",
    "\n",
    "    # create testset\n",
    "    test_data_pivot = test_data.pivot_table(index='user_id', columns='item_id', values='rating')\\\n",
    "        .reindex(sorted(df.user_id.unique()), axis=0)\\\n",
    "        .reindex(sorted(df.item_id.unique()), axis=1)\n",
    "    \n",
    "    return (train_data_pivot, test_data_pivot)\n",
    "\n",
    "\n",
    "train_data_pivot_100k, test_data_pivot_100k = split_and_pivot_dataset(df_100k, 0.2)\n",
    "\n",
    "print(\"Train Data Historgram\")\n",
    "pd.DataFrame(train_data_pivot_100k.values.flatten()).hist(bins=5, range=(1,5), density=True)\n",
    "plt.title('Train set distribution')\n",
    "print(\"Test Data Historgram\")\n",
    "pd.DataFrame(test_data_pivot_100k.values.flatten()).hist(bins=5, range=(1,5), density=True)\n",
    "plt.title('Test set distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-based prediction\n",
    "\n",
    "To predict items of interrest using an user-based approach a pairwise-distance matrix for all users (user x user) is computed. Using a cosine metric showed good results. To circumvent differences in the rating-behavior of different users the ratings have to be normalized using each users mean rating as the 0 value. By appling the score formular for user-based CF a prediction matrix is calculated. This matrix shows the predicted ratings for all items and users.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(ratings):\n",
    "    # calculate pairwise distances for users is calculated with cosine metric\n",
    "    similarity = pairwise_distances(ratings.fillna(0), metric=\"cosine\")\n",
    "    # calculate mean values \n",
    "    mean_user_rating = ratings.mean(axis=1)\n",
    "    # normalize ratings\n",
    "    norm_ratings = (ratings.fillna(0) - mean_user_rating[:, np.newaxis])\n",
    "    # calculate user-item correlations\n",
    "    prediction = mean_user_rating[:, np.newaxis] + similarity.dot(norm_ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    prediction = prediction + mean_user_rating[:, np.newaxis]\n",
    "    return prediction\n",
    "\n",
    "user_prediction_100k = predict_user(train_data_pivot_100k)\n",
    "\n",
    "pd.DataFrame(pd.DataFrame(user_prediction_100k).values.flatten()).hist(bins=5, range=(1,5), density=True)\n",
    "plt.title('User-based prediction distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Item-based prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_item(ratings):\n",
    "    # calculate pairwise distances for users to items with cosine metric\n",
    "    similarity = pairwise_distances(ratings.fillna(0).transpose(), metric=\"cosine\")\n",
    "    # calculate item-item correlations\n",
    "    pred = ratings.fillna(0).values[:, np.newaxis] + ratings.fillna(0).values.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred\n",
    "\n",
    "item_prediction_100k = predict_item(train_data_pivot_100k)\n",
    "\n",
    "#pd.DataFrame(pd.DataFrame(item_prediction_100k).values.flatten()).hist(density=True)\n",
    "#plt.title('User-based prediction distribution')\n",
    "pd.DataFrame(item_prediction_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pairwise distances for items with cosine metric\n",
    "data = train_data_pivot_100k.fillna(0)\n",
    "data_i = train_data_pivot_100k.fillna(0).transpose()\n",
    "similarity = pairwise_distances(data.transpose(), metric=\"cosine\")\n",
    "#similarity[train_data_pivot_100k == np.nan] = 0\n",
    "# calculate item-item correlations\n",
    "# (sim(Thor,Avengers) * R(Alex,Avengers) + sim(Thor, Iron man) * R(Alex, Iron man)) / (sim(Thor, Avengers) + sim(Thor, Iron man)) \n",
    "pred = similarity.dot(train_data_pivot_100k.transpose().fillna(0).values) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "\n",
    "#u_i = train_data_pivot_100k.values[0]\n",
    "#u_i2 = u_i[~np.isnan(u_i)] * similarity[~np.isnan(u_i)]\n",
    "\n",
    "pd.DataFrame(pd.DataFrame(pred).values.flatten()).hist(density=True)\n",
    "plt.title('User-based prediction distribution')\n",
    "pd.DataFrame(similarity.flatten()).hist(density=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(ratings):\n",
    "    R = ratings.values.astype(float)\n",
    "    U, S, Vt = svds(R, k=20)\n",
    "    Sd = np.diag(S)\n",
    "    return np.dot(np.dot(U, Sd), Vt)\n",
    "\n",
    "model_prediction_100k = predict_model(train_data_pivot_100k)\n",
    "model_prediction_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User-based prediction 100k runtime:\")\n",
    "%timeit predict_user(train_data_pivot_100k)\n",
    "print(\"Item-based prediction  100k runtime:\")\n",
    "%timeit predict_item(train_data_pivot_100k)\n",
    "print(\"Model-based prediction 100k runtime:\")\n",
    "%timeit predict_model(train_data_pivot_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicion = user_prediction_100k[test_data_pivot_100k.values != 0].flatten()\n",
    "groud_truth = test_data_pivot_100k.values[test_data_pivot_100k.values != 0].flatten()\n",
    "\n",
    "def mse(predicion, groud_truth):\n",
    "    predicion = predicion[~np.isnan(groud_truth)].flatten()\n",
    "    groud_truth = groud_truth[~np.isnan(groud_truth)].flatten()\n",
    "    \n",
    "    return mean_squared_error(predicion, groud_truth)\n",
    "\n",
    "print(\"User-based prediction  100k MSE: %f\" % mse(user_prediction_100k, test_data_pivot_100k.values))\n",
    "#print(\"Item-based prediction  100k MSE: %f\" % mse(item_prediction_100k, test_data_pivot_100k.values))\n",
    "#print(\"Model-based prediction 100k MSE: %f\" % mse(model_prediction_100k, test_data_pivot_100k.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = get_content_file(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\")\n",
    "with ZipFile(file_path).open(\"ml-1m/ratings.dat\") as decompressed_file:\n",
    "    df_1m = pd.read_csv(\n",
    "        decompressed_file,\n",
    "        names=data_attributes,\n",
    "        sep=\"::\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test-set (20% = 80:20) and create \"user x item = rating\" pivot table\n",
    "train_data_pivot_1m, test_data_pivot_1m = split_and_pivot_dataset(df_1m, 0.2)\n",
    "(train_data_pivot_1m.shape, test_data_pivot_1m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction_1m = predict_user(train_data_pivot_1m)\n",
    "user_prediction_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction_1m = predict_item(train_data_pivot_1m)\n",
    "item_prediction_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction_1m = predict_model(train_data_pivot_1m)\n",
    "model_prediction.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User-based prediction 1M runtime:\")\n",
    "%timeit predict_user(train_data_pivot_1m)\n",
    "print(\"Item-based prediction  1M runtime:\")\n",
    "%timeit predict_item(train_data_pivot_1m)\n",
    "print(\"Model-based prediction 1M runtime:\")\n",
    "%timeit predict_model(train_data_pivot_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User-based prediction  1M MSE: %f\" % mse(user_prediction_1m, test_data_pivot_1m.values))\n",
    "print(\"Item-based prediction  1M MSE: %f\" % mse(item_prediction_1m, test_data_pivot_1m.values))\n",
    "print(\"Model-based prediction 1M MSE: %f\" % mse(model_prediction_1m, test_data_pivot_1m.values))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitfc76f8d388764e2f87f6147686064c95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}